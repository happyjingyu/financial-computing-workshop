{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Coding AI Agents in Python\n",
    "\n",
    "#### By Pedro Izquierdo Lehmann\n",
    "\n",
    "Welcome to this hands-on introduction to **LangChain**! This notebook will guide you through building intelligent AI agents that can use tools, remember conversations, and make decisions autonomously.\n",
    "\n",
    "**What is LangChain?**\n",
    "LangChain is a framework for developing applications powered by language models. With it you can build explicit **chains**, which is an abstraction of an algorithm involving LLMs calls. Also, LangChain promotes implicit chains: instead of just asking an LLM questions, you can give it **tools** to use, and it will intelligently decide when and how to use them to answer your questions, instead of writing complex routing logic. \n",
    "\n",
    "LangChain works with the abstraction of the objects involved in the agentic system, such as\n",
    "\n",
    "- **Chains**: Abstraction of an algorithm involving multiple steps; a reusable workflow.\n",
    "- **Agents**: Abstraction of an LLM model equipped with tools, which can decide which tools/steps to run (an implicit chain).\n",
    "- **Tools**: Wrapped Python functions so the agent can call them.\n",
    "- **Memory/State**: Abstraction of context across conversation.\n",
    "\n",
    "LangChain orders these in **layers** of abstraction, so you can start simple and add power only when you need it. Each layer builds on the previous one. This notebook follows that same progression: we start with tools, then add memory, context, and structured outputs.\n",
    "\n",
    "**Content:**\n",
    "- Creating your first AI agent\n",
    "- Building custom tools for agents to use\n",
    "- Adding memory so agents remember past conversations\n",
    "- Using structured output for consistent responses\n",
    "- Context-aware tools that access user information\n",
    "- Best practices for production-ready agents\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (0. Environment Setup)\n",
    "\n",
    "Before starting, you need to set up a Python virtual environment and install all required dependencies. Follow these steps:\n",
    "\n",
    "#### 1. Create a Virtual Environment\n",
    "\n",
    "Open your terminal and navigate to the **directory containing this notebook**, then run:\n",
    "\n",
    "```bash\n",
    "python3 -m venv lang-chain\n",
    "```\n",
    "\n",
    "This creates a virtual environment in a folder called `lang-chain`.\n",
    "\n",
    "#### 2. Activate the Virtual Environment\n",
    "\n",
    "**On macOS/Linux:**\n",
    "```bash\n",
    "source lang-chain/bin/activate\n",
    "```\n",
    "\n",
    "**On Windows:**\n",
    "```bash\n",
    "lang-chain\\Scripts\\activate\n",
    "```\n",
    "\n",
    "You should see `(lang-chain)` at the beginning of your terminal prompt, indicating the virtual environment is active.\n",
    "\n",
    "#### 3. Install Required Dependencies\n",
    "\n",
    "With the virtual environment activated, install all necessary packages:\n",
    "\n",
    "```bash\n",
    "pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "```\n",
    "\n",
    "This will install:\n",
    "- `langchain` - The core LangChain framework\n",
    "- `langgraph` - For building stateful agent workflows and checkpointers\n",
    "- `langchain-anthropic` - Anthropic (Claude) model provider\n",
    "- `langchain-openai` - OpenAI model provider\n",
    "- `jupyter` - Jupyter notebook environment\n",
    "- `ipykernel` - Jupyter kernel for the virtual environment\n",
    "\n",
    "Register the virtual environment as a Jupyter kernel:\n",
    "\n",
    "```bash\n",
    "python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "```\n",
    "\n",
    "This ensures Jupyter can use your virtual environment's Python interpreter.\n",
    "\n",
    "#### 4. Start Jupyter Notebook\n",
    "\n",
    "We recommend two options to run the notebook:\n",
    "\n",
    "**Jupyter Notebook:**\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "This will open Jupyter in your web browser. Navigate to and open this notebook (`LangChain.ipynb`).\n",
    "\n",
    "**Code Editor like VS Code or Cursor:**\n",
    "\n",
    "1. Open the notebook file (`LangChain.ipynb`) in your code editor\n",
    "2. The editor should automatically detect it as a Jupyter notebook\n",
    "3. When prompted to select a kernel, choose **Python (lang-chain)** from the list\n",
    "4. If the kernel doesn't appear, you may need to refresh the kernel list or ensure the virtual environment is properly registered\n",
    "\n",
    "#### 5. Deactivate\n",
    "\n",
    "Don't forget to deactivate the virtual environment when you're done working with the following command:\n",
    "\n",
    "```bash\n",
    "deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m venv lang-chain\n",
    "# source lang-chain/bin/activate\n",
    "# lang-chain\\Scripts\\activate\n",
    "# pip install langchain langgraph langchain-anthropic langchain-openai jupyter ipykernel\n",
    "# python -m ipykernel install --user --name=lang-chain --display-name \"Python (lang-chain)\"\n",
    "# jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "# Set API key\n",
    "api_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chains\n",
    "\n",
    "A **chain** is a sequence of steps (prompts, tools, or other chains) connected into a **single reusable pipeline**.\n",
    "\n",
    "- Think of it as a recipe: each step transforms the input and passes it to the next.\n",
    "- Chains can be simple (prompt -> LLM) or complex (multi-step reasoning + tools).\n",
    "- Agents *use* chains internally, but chains are **deterministic**: the steps are predefined.\n",
    "\n",
    "LangChain lets you build **chains explicitly** (deterministic pipelines) or **implicitly** through agents (dynamic pipelines).\n",
    "\n",
    "- **Explicit chain:** You wire together steps (prompt → model → parsing). The flow is fixed and repeatable.\n",
    "- **Agentic (implicit) chain:** The model decides which steps/tools to run at runtime. The flow can vary across calls. \n",
    "\n",
    "> **Note**: The cool thing about agent chains is that instead of just asking an LLM a question, you can give it **tools** to use, and it will decide when and how to use them to answer your question. For example, you don't need to write code that says \"if the user asks about weather, call the weather tool.\" The agent figures this out on its own!\n",
    "\n",
    "Below is an example of an explicit chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: options pricing\n",
      "- Options pricing involves determining the fair value of a financial derivative based on the underlying asset's price and other market factors.  \n",
      "- The Black-Scholes model is a widely used mathematical framework for calculating the theoretical price of European-style options.  \n",
      "- Factors such as volatility, time to expiration, interest rates, and the underlying asset's price significantly influence the option's premium.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Explicit chain: fixed four-step pipeline\n",
    "# Step 1: Prompt template\n",
    "chain_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\", # sets the global rule for output format: title line + exactly three bullets. It’s treated as higher‑priority instructions.\n",
    "        \"Return output with first line 'Title: {topic}', followed by exactly three bullet points.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", # supplies the task input (the specific topic) and adds a content constraint (bullets must be full sentences).\n",
    "        \"Topic: {topic}. Each bullet must be a complete sentence.\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Step 2: Model call\n",
    "chain_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Step 3: Parse to string\n",
    "parser = StrOutputParser() # converts the raw LLM output into a string.\n",
    "\n",
    "# Step 4: Deterministic formatting\n",
    "format_output = RunnableLambda(\n",
    "    lambda s: \"\\n\".join(\n",
    "        [line for line in [\n",
    "            (\"Title: options pricing\" if not s.strip().split(\"\\n\")[0].startswith(\"Title:\") else s.strip().split(\"\\n\")[0]),\n",
    "            *[\n",
    "                (line if line.strip().startswith(\"-\") else f\"- {line.strip()}\")\n",
    "                for line in s.strip().split(\"\\n\")[1:]\n",
    "                if line.strip()\n",
    "            ][:3]\n",
    "        ] if line]\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = chain_prompt | chain_model | parser | format_output\n",
    "result = chain.invoke({\"topic\": \"options pricing\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build an Explicit Chain\n",
    "\n",
    "Create a chain that produces **three bullet points** about a finance topic. Use an explicit prompt + model pipeline, then invoke it.\n",
    "\n",
    "Hint: Use `ChatPromptTemplate`, compose with `|`, and access `response.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Risk-neutral pricing involves valuing assets by assuming investors are indifferent to risk, using a risk-neutral probability measure.  \n",
      "- It simplifies option and derivative valuation by discounting expected payoffs at the risk-free rate.  \n",
      "- This approach is fundamental in modern financial mathematics, particularly in the Black-Scholes model and other derivative pricing frameworks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# EXERCISE: Build an explicit chain\n",
    "# 1. Create a ChatPromptTemplate with a {topic} variable\n",
    "# 2. Initialize a model with temperature=0\n",
    "# 3. Compose the chain with |\n",
    "# 4. Invoke it with topic=\"risk-neutral pricing\"\n",
    "# 5. Print response.content\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise assistant.\"),\n",
    "    (\"human\", \"Provide three bullet points about: {topic}\")\n",
    "])\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"gpt-4.1-nano-2025-04-14\",\n",
    "    temperature=0,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "response = chain.invoke({\"topic\": \"risk-neutral pricing\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following will introduce chains implicitly as we build tool-driven workflows, then show how agents extend them with decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating an Agent\n",
    "\n",
    "Let's start by creating a simple agent. You can think of an agent as a **chain with decision-making**: it interprets the user input, decides which tools to call (if any), and produces a final response.\n",
    "\n",
    "An agent needs:\n",
    "1. A **model** (the LLM that does the thinking)\n",
    "2. **Tools** (functions the agent can call)\n",
    "3. A **system prompt** (instructions for the agent)\n",
    "\n",
    "Here's a concrete example of how to create and use an agent with a time tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The current time in UTC is 12:18 PM on January 15, 2026.\n"
     ]
    }
   ],
   "source": [
    "# Example: Create an agent with a time tool\n",
    "def get_current_time(timezone: str = \"UTC\") -> str:\n",
    "    \"\"\"Get the current time in a specified timezone.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"Current time in {timezone}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Initialize the model\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "\n",
    "# Create the agent\n",
    "example_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_current_time],\n",
    "    system_prompt=\"You are a helpful time assistant\"\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "example_response = example_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what time is it?\"}]}\n",
    ")\n",
    "\n",
    "# Access the response\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Basic Agent\n",
    "\n",
    "Now it's your turn! Create your first agent following the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco is always sunny!\n"
     ]
    }
   ],
   "source": [
    "# First, let's create a simple tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# EXERCISE: Create a basic agent with a weather tool\n",
    "# 1. Initialize a chat model\n",
    "# Hint: Use init_chat_model() from langchain.chat_models with model name \"gpt-4.1-nano-2025-04-14\" or \"gpt-4\"\n",
    "model = init_chat_model(\n",
    "    \"gpt-4.1-nano-2025-04-14\",  # or \"gpt-4o-mini\"\n",
    "    temperature=0,\n",
    "    api_key = api_key\n",
    ")\n",
    "\n",
    "# 2. Create an agent using create_agent\n",
    "# Hint: Use create_agent() from langchain.agents with model, tools=[get_weather], and system_prompt=\"You are a helpful assistant\"\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\"\n",
    ")\n",
    "\n",
    "# 3. Run the agent with a message asking about the weather in San Francisco\n",
    "# Hint: Use agent.invoke() with {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in San Francisco\"}]}\n",
    "response = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"what is the weather in San Francisco\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the response\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Custom Tools\n",
    "\n",
    "Tools are functions that agents can call. LangChain makes it easy to convert Python functions into tools using the `@tool` decorator. The `@tool` decorator:\n",
    "- Automatically extracts function name, description, and parameters\n",
    "- Makes the function available to the agent\n",
    "- Handles type validation and conversion\n",
    "\n",
    "> **Important**: The function's docstring becomes part of the agent's prompt! Make it descriptive so the agent knows when to use the tool.\n",
    "\n",
    "Here's a working example using different tools (string manipulation) to demonstrate the @tool decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: The reversed string is \"dlroW olleH\" and it contains 2 words.\n"
     ]
    }
   ],
   "source": [
    "# Example: Create tools for string manipulation (different from the calculator exercise)\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"Reverse a string.\"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "@tool\n",
    "def uppercase_string(text: str) -> str:\n",
    "    \"\"\"Convert a string to uppercase.\"\"\"\n",
    "    return text.upper()\n",
    "\n",
    "@tool\n",
    "def count_words(text: str) -> int:\n",
    "    \"\"\"Count the number of words in a string.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Create an agent with these string manipulation tools\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "string_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[reverse_string, uppercase_string, count_words],\n",
    "    system_prompt=\"You are a helpful text processing assistant\"\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "example_response = string_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Reverse the string 'Hello World' and count its words\"}]}\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create a LangChain tools\n",
    "\n",
    "Now it's your turn! Create your first LangChain tools following the syntaxis of the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, 15 plus 27 equals 42. Then, multiplying 42 by 3 gives 126.\n"
     ]
    }
   ],
   "source": [
    "### Exercise 2: Create Multiple Tools\n",
    "\n",
    "# EXERCISE: Create a calculator agent with multiple tools\n",
    "# 1. Create a tool for addition\n",
    "# Hint: Use @tool decorator from langchain.tools, function should take (a: float, b: float) and return a + b\n",
    "# 1. Create a tool for addition\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 2. Create a tool for multiplication\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# 3. Create a tool for getting the square root\n",
    "@tool\n",
    "def sqrt(x: float) -> float:\n",
    "    \"\"\"Get the square root of a number.\"\"\"\n",
    "    return math.sqrt(x)\n",
    "\n",
    "# 4. Create an agent with all three tools\n",
    "calculator_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[add, multiply, sqrt],\n",
    "    system_prompt=\"You are a helpful calculator assistant\"\n",
    ")\n",
    "\n",
    "# 5. Test your agent\n",
    "response = calculator_agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 15 plus 27, then multiply that by 3?\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Behavior**: The agent should:\n",
    "1. First call `add(15, 27)` to get 42\n",
    "2. Then call `multiply(42, 3)` to get 126\n",
    "3. Return the final answer\n",
    "\n",
    "This demonstrates that agents can **chain multiple tool calls** to solve complex problems! The agent automatically figures out the sequence of operations needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tools with Runtime Context\n",
    "\n",
    "Sometimes tools need access to runtime information (like user IDs, session data, etc.). LangChain provides `ToolRuntime` for this. `ToolRuntime` allows tools to access:\n",
    "- **Context**: Custom data passed when invoking the agent\n",
    "- **Memory**: Conversation history and state\n",
    "- **Configuration**: Runtime settings\n",
    "\n",
    "> **Note**: The `ToolRuntime` parameter is automatically injected by LangChain. You don't pass it when calling the tool - LangChain handles that for you!\n",
    "\n",
    "Here is an example that uses `ToolRuntime` to build Context-Aware Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example response: Based on your preferences, I recommend items in blue color. Would you like to see some specific options or categories?\n"
     ]
    }
   ],
   "source": [
    "# # Example: Context-aware tool for user preferences (different from greeting exercise)\n",
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Define a context schema with user preferences\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "    favorite_color: str\n",
    "\n",
    "# Create a tool that uses ToolRuntime to access user preferences\n",
    "@tool\n",
    "def get_recommendation(runtime: ToolRuntime[UserContext]) -> str:\n",
    "    \"\"\"Get a personalized recommendation based on user preferences.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    favorite_color = runtime.context.favorite_color\n",
    "    return f\"User {user_id} might like items in {favorite_color} color!\"\n",
    "\n",
    "# Create an agent with this tool\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "recommendation_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_recommendation],\n",
    "    system_prompt=\"You are a helpful recommendation assistant\",\n",
    "    context_schema=UserContext\n",
    ")\n",
    "\n",
    "# Invoke with context\n",
    "example_response = recommendation_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What would you recommend for me?\"}]},\n",
    "    context=UserContext(user_id=\"123\", favorite_color=\"blue\")\n",
    ")\n",
    "print(\"Example response:\", example_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Context-Aware Tools\n",
    "\n",
    "Now create your own personalized greeting tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Bill! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# 1. Define a Context dataclass with a user_name field\n",
    "@dataclass\n",
    "class Context:\n",
    "    user_name: str\n",
    "\n",
    "# 2. Create a tool that uses ToolRuntime to access context\n",
    "@tool\n",
    "def get_personalized_greeting(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Get a personalized greeting for the user.\"\"\"\n",
    "    user_name = runtime.context.user_name\n",
    "    return f\"Hello, {user_name}! How can I help you today?\"\n",
    "\n",
    "# 3. Create an agent with this tool\n",
    "# Assuming model is already initialized as in the example\n",
    "personalized_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[get_personalized_greeting],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    "    context_schema=Context\n",
    ")\n",
    "\n",
    "# 4. Invoke the agent with context\n",
    "response = personalized_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is your name?\"}]},\n",
    "    context=Context(user_name=\"Bill\")\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding Memory to Agents\n",
    "\n",
    "So far, our agents don't remember previous conversations. Let's add **memory** so agents can maintain context across multiple interactions. A **checkpointer** stores conversation state, which you can think of as the **state of the agent's chain across turns**. LangChain provides:\n",
    "- `InMemorySaver`: For development/testing (lost when program ends)\n",
    "- Database checkpointers: For production (persistent storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: `ToolRuntime` and `InMemorySaver` both relate to “runtime context,” but they operate at different layers.\n",
    ">- `ToolRuntime` is per-tool-call and injected into tool functions; it provides a view of context/memory/config at that moment.\n",
    ">- `InMemorySaver` is storage, passed to the agent as a checkpointer to persist conversation state between invocations (keyed by `thread_id`). It does not get injected into tools.\n",
    ">- `ToolRuntime` doesn’t store anything by itself; `InMemorySaver` doesn’t provide arbitrary runtime context/config—only persistence for state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to add memory to an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: Here's an inspiring quote about success: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n",
      "Second response: I shared the quote: \"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Create a checkpointer\n",
    "example_checkpointer = InMemorySaver()\n",
    "\n",
    "# Create a quote tool (different from the fact tool in the exercise)\n",
    "@tool\n",
    "def get_quote(category: str) -> str:\n",
    "    \"\"\"Get an inspirational quote by category.\"\"\"\n",
    "    quotes = {\n",
    "        \"success\": \"Success is not final, failure is not fatal: it is the courage to continue that counts.\",\n",
    "        \"wisdom\": \"The only true wisdom is in knowing you know nothing.\",\n",
    "        \"motivation\": \"The way to get started is to quit talking and begin doing.\"\n",
    "    }\n",
    "    return quotes.get(category.lower(), \"Here's a quote: Keep moving forward!\")\n",
    "\n",
    "# Create an agent with memory\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "quote_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_quote],\n",
    "    system_prompt=\"You are a helpful assistant that shares inspirational quotes\",\n",
    "    checkpointer=example_checkpointer\n",
    ")\n",
    "\n",
    "# Create a config with thread_id\n",
    "example_config = {\"configurable\": {\"thread_id\": \"quote-session-1\"}}\n",
    "\n",
    "# First message\n",
    "example_response1 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Give me a quote about success\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "\n",
    "# Second message - agent remembers!\n",
    "example_response2 = quote_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What quote did you just share?\"}]},\n",
    "    config=example_config\n",
    ")\n",
    "print(\"First response:\", example_response1['messages'][-1].content)\n",
    "print(\"Second response:\", example_response2['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: The `thread_id` in the config is crucial! It tells the checkpointer which conversation to load. Different `thread_id` values mean different conversations. This allows you to manage multiple concurrent conversations with the same agent.\n",
    "\n",
    "### Exercise 4: Conversational Memory\n",
    "\n",
    "Now create your own agent with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: A fact about Python is that it was named after Monty Python's Flying Circus.\n",
      "Second response: The fact I just told you is that Python was named after Monty Python's Flying Circus.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.tools import tool\n",
    "\n",
    "# 1. Create an InMemorySaver checkpointer\n",
    "# This object stores the conversation state in memory.\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# 2. Create a simple tool that returns a fact\n",
    "@tool\n",
    "def get_fact(topic: str) -> str:\n",
    "    \"\"\"Get an interesting fact about a topic.\"\"\"\n",
    "    facts = {\n",
    "        \"python\": \"Python was named after Monty Python's Flying Circus\",\n",
    "        \"ai\": \"The term 'artificial intelligence' was coined in 1956\",\n",
    "        \"space\": \"A day on Venus is longer than its year\"\n",
    "    }\n",
    "    return facts.get(topic.lower(), f\"I don't know much about {topic}\")\n",
    "\n",
    "# 3. Create an agent with the checkpointer\n",
    "# We pass the checkpointer instance to the agent to enable persistence.\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "memory_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_fact],\n",
    "    system_prompt=\"You are a helpful assistant that can remember facts\",\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# 4. Create a config with a thread_id (this identifies the conversation)\n",
    "# The thread_id tells the checkpointer which specific history to load.\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "\n",
    "# 5. Ask the agent: \"Tell me a fact about Python\"\n",
    "response1 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a fact about Python\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(\"First response:\", response1['messages'][-1].content)\n",
    "\n",
    "# 6. In a follow-up message, ask: \"What was the fact you just told me?\"\n",
    "# By using the same config/thread_id, the agent recalls the previous turn.\n",
    "response2 = memory_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What was the fact you just told me?\"}]},\n",
    "    config=config\n",
    ")\n",
    "print(\"Second response:\", response2['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structured Output\n",
    "\n",
    "Sometimes you want the agent's response in a specific format. LangChain supports **structured output** using dataclasses or `Pydantic` models. Its functional object for this is `ToolStrategy`, which tells the agent to use tools AND return structured output. The agent will still use tools, but format its final response according to your schema. This gives you the best of both worlds - tool usage with predictable output formats.\n",
    "\n",
    "Here's how to create an agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: Laptop\n",
      "Price: 999.0\n",
      "Rating: 4.5\n",
      "Features: ['High performance', 'Lightweight design', 'Long battery life']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a different response format\n",
    "@dataclass\n",
    "class ProductRecommendation:\n",
    "    \"\"\"Response schema for product recommendations.\"\"\"\n",
    "    product_name: str\n",
    "    price: float\n",
    "    rating: float = 0.0\n",
    "    features: list[str] = field(default_factory=list)\n",
    "\n",
    "# Create a product search tool\n",
    "@tool\n",
    "def search_products(category: str) -> str:\n",
    "    \"\"\"Search for products in a category.\"\"\"\n",
    "    return f\"Found products in {category}: Laptop ($999, 4.5 stars), Tablet ($499, 4.2 stars)\"\n",
    "\n",
    "# Create an agent with structured output\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "product_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[search_products],\n",
    "    system_prompt=\"You are a helpful product recommendation assistant\",\n",
    "    response_format=ToolStrategy(ProductRecommendation)\n",
    ")\n",
    "\n",
    "# Ask a question and get a structured response\n",
    "example_response = product_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Recommend a laptop for me\"}]}\n",
    ")\n",
    "\n",
    "# Access the structured response\n",
    "structured = example_response['structured_response']\n",
    "print(\"Product Name:\", structured.product_name)\n",
    "print(\"Price:\", structured.price)\n",
    "print(\"Rating:\", structured.rating)\n",
    "print(\"Features:\", structured.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Structured Responses\n",
    "\n",
    "Now create your own agent with structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The capital of France is Paris.\n",
      "Confidence: 0.0\n",
      "Sources: []\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from langchain.tools import tool\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "# 1. Define a ResponseFormat dataclass\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"The structured response schema for the agent's final answer.\"\"\"\n",
    "    answer: str\n",
    "    confidence: float = 0.0\n",
    "    sources: list[str] = field(default_factory=list)\n",
    "\n",
    "# 2. Create a simple tool\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search the internal knowledge base for information.\"\"\"\n",
    "    # Simulating a knowledge base lookup\n",
    "    return \"The capital of France is Paris. Source: World Atlas 2024.\"\n",
    "\n",
    "# 3. Create an agent with structured output\n",
    "# Using the ToolStrategy to bridge the model and our dataclass\n",
    "example_model = init_chat_model(\"gpt-4.1-nano-2025-04-14\", temperature=0, api_key=api_key)\n",
    "structured_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[search_knowledge_base],\n",
    "    system_prompt=\"You are a factual research assistant.\",\n",
    "    response_format=ToolStrategy(ResponseFormat)\n",
    ")\n",
    "\n",
    "# 4. Ask a question and get a structured response\n",
    "response = structured_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]}\n",
    ")\n",
    "\n",
    "# Access the structured response from the result dictionary\n",
    "structured = response['structured_response']\n",
    "print(f\"Answer: {structured.answer}\")\n",
    "print(f\"Confidence: {structured.confidence}\")\n",
    "print(f\"Sources: {structured.sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guided Exercise: Daily S&P 500 Decision with Twitter Sentiment Analysis\n",
    "\n",
    "In this exercise, you will build an agent that decides whether to **BUY** or **NOT BUY** units of the S&P 500 (e.g., SPY), computing **local sentiment** for tweets **before the decision day**. We use the following Hugging Face dataset: https://huggingface.co/datasets/StephanAkkerman/stock-market-tweets-data\n",
    "\n",
    "> **Note**: This is a simplified educational example; don’t take it as financial advice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925ff277efcb40c3bcebb9282061bd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\happy\\fcw\\financial-computing-workshop\\lang-chain\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\happy\\.cache\\huggingface\\hub\\datasets--StephanAkkerman--stock-market-tweets-data. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d70d3c776b64c119c2e2e0051d9c53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stock-market-tweets-data.csv:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510f377ba7dc4556bf414f1bcf0b9740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/923673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Build a local pandas dataframe (sampled)\n",
    "ds = load_dataset(\"StephanAkkerman/stock-market-tweets-data\", split=\"train\")\n",
    "df = ds.select(range(20000)).to_pandas()\n",
    "\n",
    "# Normalize and parse dates\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\", utc=True)\n",
    "df = df.dropna(subset=[\"created_at\", \"text\"])\n",
    "df[\"created_at_date\"] = df[\"created_at\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentiment score is 0.1207, which is positive. Based on this positive sentiment, I recommend to BUY SPY. The themes in the tweets suggest a generally optimistic outlook among investors.\n"
     ]
    }
   ],
   "source": [
    "# Local sentiment scoring\n",
    "positive_words = {\"gain\", \"gains\", \"bull\", \"bullish\", \"up\", \"upgrade\", \"beat\", \"strong\", \"rally\", \"surge\", \"record\"}\n",
    "negative_words = {\"loss\", \"losses\", \"bear\", \"bearish\", \"down\", \"downgrade\", \"miss\", \"weak\", \"selloff\", \"drop\", \"plunge\"}\n",
    "\n",
    "def sentiment_score(text: str) -> int:\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "    score = sum(1 for t in tokens if t in positive_words) - sum(1 for t in tokens if t in negative_words)\n",
    "    return score\n",
    "\n",
    "df[\"sentiment_score\"] = df[\"text\"].fillna(\"\").apply(sentiment_score)\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "# 1. Define the sentiment summary tool\n",
    "@tool\n",
    "def get_sentiment_summary() -> str:\n",
    "    \"\"\"Summarize local tweet sentiment across the full dataset.\"\"\"\n",
    "    # Compute the average sentiment from the dataframe\n",
    "    avg_score = df[\"sentiment_score\"].mean()\n",
    "    total_tweets = len(df)\n",
    "    \n",
    "    # Return a string containing the average score as required\n",
    "    return f\"The average sentiment score is {avg_score:.4f} based on {total_tweets} tweets.\"\n",
    "\n",
    "# 2. Write the SYSTEM_PROMPT\n",
    "# Include goals and rules, ensuring the agent knows how to interpret the score.\n",
    "SYSTEM_PROMPT = \"\"\"You are a financial analyst agent. \n",
    "Your goal is to decide whether to BUY or NOT BUY SPY based on tweet sentiment data.\n",
    "\n",
    "Rules:\n",
    "1. Always use the 'get_sentiment_summary' tool to retrieve the current sentiment.\n",
    "2. If the average sentiment score is positive (> 0), recommend 'BUY'.\n",
    "3. If the average sentiment score is zero or negative (<= 0), recommend 'NOT BUY'.\n",
    "4. Your final response must explicitly state the average sentiment score and provide a brief justification based on the themes you observe.\"\"\"\n",
    "\n",
    "# 3. Create the agent\n",
    "# Using the model, tool, and prompt defined above.\n",
    "decision_agent = create_agent(\n",
    "    model=example_model,\n",
    "    tools=[get_sentiment_summary],\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "# 4. Invoke the agent\n",
    "response = decision_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Use the overall tweet sentiment to decide BUY or NOT BUY SPY.\"}]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Congratulations \n",
    "You've completed the LangChain tutorial! We covered\n",
    "\n",
    "- How to create agents with LangChain  \n",
    "- How to build custom tools  \n",
    "- How to add memory to agents  \n",
    "- How to use structured output  \n",
    "- How to build a daily decision agent  \n",
    "\n",
    "### Possible next steps to explore\n",
    "   - **LangGraph**: For more complex agent workflows (see the LangGraph notebook!)\n",
    "   - **Retrieval**: Connect agents to vector databases for RAG\n",
    "   - **Multi-agent systems**: Agents that collaborate\n",
    "   - **LangSmith**: Observability and debugging tools\n",
    "\n",
    "### Additional resources\n",
    "   - [LangChain Docs](https://docs.langchain.com)\n",
    "   - [LangChain Quickstart](https://docs.langchain.com/oss/python/langchain/quickstart)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Happy learning!\n",
    "\n",
    "Pedro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-chain)",
   "language": "python",
   "name": "lang-chain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
